{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG_model 복사 및 리사이즈 테스트.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNjqWOxToUlMPUSwl8JG7SX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hellog2n/Example-Tensorflow-Python/blob/main/VGG_model_%EB%B3%B5%EC%82%AC_%EB%B0%8F_%EB%A6%AC%EC%82%AC%EC%9D%B4%EC%A6%88_%ED%85%8C%EC%8A%A4%ED%8A%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atu-Smus0WJ8"
      },
      "source": [
        "# VGG Net 모델로 이미지 예측하기\n",
        "\n",
        "* VGG의 사이즈는 224*224로 고정하였습니다.\n",
        "* 이미지를 갖고와서 리사이즈 하는 방식에 따라 다른 결과값이 도출되는 것 같습니다.\n",
        "* VGG 모델의 레이어를 복사해서 사용하는 방식은 기존의 VGG 모델과 같은 결과를 출력하기에 문제가 없는 것으로 판단됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxdtg4YZ0s12"
      },
      "source": [
        "### 라이브러리 갖고오기\n",
        "### VGG 모델 임포팅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qgAZrgShTMT",
        "outputId": "445dbd6a-14da-4a46-b96f-1b76372b2395"
      },
      "source": [
        "from skimage.transform import resize\n",
        "from numpy import asarray\n",
        "from tensorflow.python.keras.applications.vgg16 import VGG16\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.applications.vgg16 import decode_predictions\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "# VGG Model\n",
        "size = 224\n",
        "\n",
        "# Default Input Size for VGG : 224*224\n",
        "vgg_model = VGG16()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prfGQvD508Gh"
      },
      "source": [
        "### Method1\n",
        "기존에 사용하고 있던 리사이즈 방식 \n",
        "\n",
        "- new_shape =  (224, 224, 3)\n",
        "- cv2 함수를 이용하여 이미지 로드\n",
        "- 이미지를 대량으로 갖고와서 skimage의 Resize 함수를 이용하여 사이즈를 바꾼다.\n",
        "- 이미지 Preprocessing 이후 모델에 입력으로 주어진다. (Method2와 같음)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4640NQ7vuQd"
      },
      "source": [
        "# -------------------------- Method 1 --------------------------------\n",
        "# Method1 이미지 갖고오기\n",
        "def load_Image(path):\n",
        "  img = cv2.cvtColor(cv2.imread(path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
        "  return img\n",
        "\n",
        "# Method1 이미지 크기 조정\n",
        "def scale_images_GPU(images, new_shape):\n",
        "    images_list = list()\n",
        "    for image in images:\n",
        "        new_image = resize(image, new_shape, 0)\n",
        "        images_list.append(new_image)\n",
        "    return asarray(images_list)\n",
        "\n",
        "# Method1 이미지 리사이즈 후 모델에 입력\n",
        "def Get_Class_Method1(inputs, model):\n",
        "    \"\"\"Compose the preprocess\"\"\"\n",
        "    \"\"\" 이미지 리사이즈 224\"\"\"\n",
        "    # GPU 설정\n",
        "    with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\n",
        "      inputs = scale_images_GPU(inputs, (224,224,3))\n",
        "      inputs = tf.keras.applications.vgg16.preprocess_input(inputs)\n",
        "      return model.predict(inputs)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r_o3qWq1bCa"
      },
      "source": [
        "### Method2\n",
        "출처 : https://machinelearningmastery.com/use-pre-trained-vgg-model-classify-objects-photographs/ \n",
        "\n",
        "img = load_img(imageName\"String\", target_size=(224, 224))\n",
        "- keras.preprocessing.image의 함수인 load_img 함수를 이용하여 사이즈를 지정하여 로드한다.\n",
        "- 이후 이미지를 array로 변환하고 Reshape 한다.\n",
        "- 이미지 Preprocessing 이후 모델에 입력으로 주어진다. (Method1와 같음)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_nqr_c5v0BU"
      },
      "source": [
        "# -------------------------- Method 2 --------------------------------\n",
        "\n",
        "# 이미지 리사이즈 후 모델에 입력\n",
        "def Get_Class_Method2(image, model):\n",
        "  with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\n",
        "    # convert the image pixels to a numpy array\n",
        "    image = img_to_array(image)\n",
        "    # reshape data for the model\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    # prepare the image for the VGG model\n",
        "    image = preprocess_input(image)\n",
        "    # predict the probability across all output classes\n",
        "    yhat = model.predict(image)\n",
        "  return yhat"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72yDHVxe2QKK"
      },
      "source": [
        "### VGG 모델 사용하기\n",
        "VGG 모델의 클래스 예측을 확인하기 위해 모델에서 레이어를 제거하지 않고 그대로 사용하였다. \n",
        "\n",
        "\n",
        "Method 방식에 따라 이미지 로드 및 리사이즈 할 방식을 다룬다.\n",
        "\n",
        "```\n",
        "embed_image_in_VGG16(이미지, 사용할 Method 방식(integer))\n",
        "```\n",
        "* VGG 모델을 복사하여 사용한다. model을 임시로 선언하고, 이 모델에 VGG 모델의 모든 레이어를 복사하여 추가한다. 복사한 모델의 경우, 모델이 예측시 Train되지 않도록 False값을 설정하였다.\n",
        "\n",
        "```\n",
        "embed_image_in_VGG162(이미지, 사용할 Method 방식(integer))\n",
        "```\n",
        "* VGG 모델을 그대로 사용한다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fmA1fOnv5vd"
      },
      "source": [
        "# 모델을 복사하여 사용한다.\n",
        "def embed_image_in_VGG16(img, method):\n",
        "    # 모델 복사\n",
        "    model = tf.keras.Sequential()\n",
        "    for layer in vgg_model.layers[:]:\n",
        "        model.add(layer)\n",
        "\n",
        "    # 마지막 레이어 제거 (Classess 1000)\n",
        "    #model.layers.pop()\n",
        "\n",
        "    # 모든 레이어가 학습되지 않도록 적용\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = False\n",
        "    if method == 1:\n",
        "      Values = Get_Class_Method1(img, model=model)\n",
        "    elif method == 2:\n",
        "      Values = Get_Class_Method2(img, model=model)\n",
        "    else:\n",
        "      raise NotImplementedError\n",
        "    return Values\n",
        "\n",
        "\n",
        "# VGG 모델 원본 그대로를 사용한다.\n",
        "def embed_image_in_VGG162(img, method):\n",
        "    if method == 1:\n",
        "      Values = Get_Class_Method1(img, model=vgg_model)\n",
        "    elif method == 2:\n",
        "      Values = Get_Class_Method2(img, model=vgg_model)\n",
        "    else:\n",
        "      raise NotImplementedError\n",
        "    return Values\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQEEDu3l4RRf"
      },
      "source": [
        "### Main Method\n",
        "\n",
        "`imageName = \"이미지_이름.확장자\" 에 모델의 입력으로 주어질 이미지의 이름과 확장자를 선언한다.`\n",
        "\n",
        "### 고양이 이미지들로 테스트를 해보았습니다.\n",
        "\n",
        "\n",
        "###예시 이미지 1\n",
        "![cat3.jpeg](https://drive.google.com/uc?export=view&id=14zn7Gt_Q8X0c5SPv-wjjA549mVPOWkjP)\n",
        "\n",
        "결과값 :\n",
        "\n",
        "Method1(VGG 복사, VGG 원본) ->\n",
        "matchstick (7.70%)\n",
        "matchstick (7.70%)\n",
        "\n",
        "Method2(VGG 복사, VGG 원본) ->\n",
        "tiger_cat (15.25%)\n",
        "tiger_cat (15.25%)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 예시 이미지 2\n",
        "\n",
        "![cat2.png](https://drive.google.com/uc?export=view&id=1bN9w3urpjDuy0ji_ZheqyDAUS9JkURq7)\n",
        "\n",
        "결과값 :\n",
        "\n",
        "Method1(VGG 복사, VGG 원본) ->\n",
        "matchstick (7.61%)\n",
        "matchstick (7.61%)\n",
        "\n",
        "Method2(VGG 복사, VGG 원본) ->\n",
        "carton (14.74%)\n",
        "carton (14.74%)\n",
        "\n",
        "---\n",
        "\n",
        "### 예시 이미지 3\n",
        "\n",
        "![cat.jpeg](https://drive.google.com/uc?export=view&id=19uLhViuBw3nOpr5Gsgr8D1SxwFjLj1Ny)\n",
        "\n",
        "결과값 :\n",
        "\n",
        "Method1(VGG 복사, VGG 원본) ->\n",
        "matchstick (7.55%)\n",
        "matchstick (7.55%)\n",
        "\n",
        "\n",
        "Method2(VGG 복사, VGG 원본) ->\n",
        "Egyptian_cat (56.56%)\n",
        "Egyptian_cat (56.56%)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcZT2XNIuQwy",
        "outputId": "a06c19ef-fa48-4ee0-e019-aef3875309c2"
      },
      "source": [
        "# cat3.jpeg\n",
        "\n",
        "imageName = \"이미지_이름.확장자\"\n",
        "\n",
        "def main(imageName):\n",
        "  Method1(imageName)\n",
        "  Method2(imageName)\n",
        "  \n",
        "\n",
        "def Method1(imageName):\n",
        "  # Method 1\n",
        "\n",
        "  path = imageName\n",
        "  img = load_Image(path)\n",
        "  \n",
        "  # VGG 복사된 모델 => 함수 매개변수 (이미지, Method1 선택)\n",
        "  yhat = embed_image_in_VGG16(img, 1)\n",
        "  label = decode_predictions(yhat)\n",
        "  label = label[0][0]\n",
        "  print(\"%s (%.2f%%)\" % (label[1], label[2] * 100))\n",
        "\n",
        "  # VGG 원본 모델 => 함수 매개변수 (이미지, Method1 선택)\n",
        "  yhat2 = embed_image_in_VGG162(img, 1)\n",
        "  label2 = decode_predictions(yhat2)\n",
        "  label2 = label2[0][0]\n",
        "  print(\"%s (%.2f%%)\" % (label2[1], label2[2] * 100))\n",
        "\n",
        "\n",
        "def Method2(imageName):\n",
        "  # Method 2  \n",
        "  img = load_img(imageName, target_size=(224, 224))\n",
        "  # VGG 복사된 모델 => 함수 매개변수 (이미지, Method2 선택)\n",
        "  yhat = embed_image_in_VGG16(img, 2)\n",
        "  label = decode_predictions(yhat)\n",
        "  label = label[0][0]\n",
        "  print(\"%s (%.2f%%)\" % (label[1], label[2] * 100))\n",
        "\n",
        "  # VGG 원본 모델 => 함수 매개변수 (이미지, Method2 선택)\n",
        "  yhat2 = embed_image_in_VGG162(img, 2)\n",
        "  label2 = decode_predictions(yhat2)\n",
        "  label2 = label2[0][0]\n",
        "  print(\"%s (%.2f%%)\" % (label2[1], label2[2] * 100))\n",
        "\n",
        "\n",
        "main()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "40960/35363 [==================================] - 0s 0us/step\n",
            "matchstick (7.70%)\n",
            "matchstick (7.70%)\n",
            "tiger_cat (15.25%)\n",
            "tiger_cat (15.25%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}